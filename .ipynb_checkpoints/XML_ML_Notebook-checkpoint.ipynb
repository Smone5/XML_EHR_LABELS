{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tubodbc for speed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = None\n",
    "#from io import StringIO\n",
    "\n",
    "from turbodbc import Megabytes, connect, make_options\n",
    "options = make_options(read_buffer_size=Megabytes(100),\n",
    "                        parameter_sets_to_buffer=1000,\n",
    "                        varchar_max_character_limit=10000,\n",
    "                        use_async_io=True,\n",
    "                        prefer_unicode=True,\n",
    "                        autocommit=True,\n",
    "                        large_decimals_as_64_bit_types=True,\n",
    "                        limit_varchar_results_to_max=True)\n",
    "\n",
    "connection = connect(dsn='PROJECT_MT_AUBURN')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = '''\n",
    "SELECT \n",
    "    ACCOUNT_NUMBER\n",
    "\t, REPLACE(DX1+','+DX2+','+DX3+','+DX4+','+DX5+','+DX6+','+DX7+','+DX8+','+DX9+','+DX10+','+DX11+','+DX12+','+DX13+','+DX14+','+DX15+','+DX16+','+DX17+','+DX18+','+DX19+','+DX20+','+DX21+','+DX22+','+DX23+','+DX24+','+DX25+','+DX26+','+DX27+','+DX28+','+DX29+','+DX30,',',' ') AS dx\n",
    "\t, REPLACE(PX1+','+PX2+','+PX3+','+PX4+','+PX5+','+PX6+','+PX7+','+PX8+','+PX9+','+PX10+','+PX11+','+PX12+','+PX13+','+PX14+','+PX15,',',' ') as px\n",
    "\t, REPLACE(CPT1+','+CPT2+','+CPT3+','+CPT4+','+CPT5+','+CPT6,',',' ') as cpt\n",
    "\t, REPLACE(MOD1+','+MOD2+','+MOD3+','+MOD4+','+MOD5+','+MOD6+','+MOD7+','+MOD8+','+MOD9+','+MOD10+','+MOD11+','+MOD12+','+MOD13+','+MOD14+','+MOD15,',',' ') as mod\n",
    "INTO ##CODES\n",
    "FROM MAH_ENCOUNTER_DETAIL\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(QUERY_1)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = '''\n",
    "IF OBJECT_ID('TEMPDB.DBO.##TEMP1','U') IS NOT NULL\n",
    "DROP TABLE ##TEMP1\n",
    "\n",
    "select\n",
    "\tservice_date,\n",
    "\tpost_date,\n",
    "\tfacility,\n",
    "\taccount,\n",
    "\tpatient_type,\n",
    "\tinsurance_plan,\n",
    "\tpayor,\n",
    "\tdescrp,\n",
    "    attending_provider,\n",
    "\tsource_ub04,\n",
    "    hcpcs,\n",
    "    mod,\n",
    "\tquant,\n",
    "\tprice,\n",
    "    target_ub04,\n",
    "    labels_dx,\n",
    "    labels_px,\n",
    "    labels_cpt,\n",
    "\tlabels_mod\n",
    "into ##TEMP1\n",
    "from (\n",
    "\t\tSELECT\n",
    "\t\t\t'MAH' as facility\n",
    "\t\t\t, CAST(SERVICE_DATE AS DATE) as service_date \n",
    "\t\t\t, CAST(POST_DATE AS DATE) AS post_date\n",
    "\t\t\t, 'MAH'+cd.ACCOUNT_NUMBER AS account\n",
    "\t\t\t, CASE WHEN ed.\"Account_Base_Class\"='Outpatient' THEN 'O' Else 'I' end as patient_type\n",
    "\t\t\t, cd.PRIM_INS_PLAN as insurance_plan\n",
    "\t\t\t, cd.PRIM_PAYR as payor\n",
    "\t\t\t, LOWER(PROC_DESC) AS descrp\n",
    "            , ed.\"Attend_Prov\" as attending_provider\n",
    "            , cds.dx as labels_dx\n",
    "            , cds.px as labels_px\n",
    "            , cds.cpt as labels_cpt\n",
    "            , cds.mod as labels_mod\n",
    "\t\t\t, RIGHT(REV_CODE, 3) AS source_ub04\n",
    "\t\t\t, ubeditor.\"Recommendation\" as target_ub04\n",
    "\t\t\t, QUANTITY AS quant\n",
    "\t\t\t, AMOUNT as price\n",
    "            , LEFT(RIGHT(\"CPT_CODE\", 6), 5) as hcpcs\n",
    "            , CASE WHEN MODIFIER='' THEN NULL ELSE MODIFIER END AS mod\n",
    "\t\tFROM PROJECT_MT_AUBURN..MAH_CHARGE_DETAIL cd\n",
    "\t\tLEFT JOIN CDM..UB_Editor_Recommendations_2017 ubeditor\n",
    "\t\t\ton ubeditor.\"HCPCS\" = LEFT(RIGHT(\"CPT_CODE\", 6), 5)\n",
    "\t\tLEFT JOIN PROJECT_MT_AUBURN..[MAH_ENCOUNTER_DETAIL] ed\n",
    "\t\t\ton ed.\"ACCOUNT_NUMBER\" = cd.\"ACCOUNT_NUMBER\"\n",
    "        LEFT JOIN ##CODES cds\n",
    "            on cds.\"ACCOUNT_NUMBER\" = cd.\"ACCOUNT_NUMBER\"\n",
    "\t\tWHERE CPT_CODE IN (select \"HCPCS Code\" from CDM..ADDENDUMB_2018)\n",
    "\t\t\tAND QUANTITY > 0\n",
    "\t\t\tAND AMOUNT > 0\n",
    "\t\t\tAND \"Modifier\"  NOT LIKE '%[0-9]%'\n",
    "\t\tGROUP BY\n",
    "\t\t\tCAST(SERVICE_DATE AS DATE) \n",
    "\t\t\t, CAST(POST_DATE AS DATE)\n",
    "\t\t\t, cd.ACCOUNT_NUMBER\n",
    "\t\t\t, ed.\"Account_Base_Class\"\n",
    "\t\t\t, cd.PRIM_INS_PLAN\n",
    "\t\t\t, cd.PRIM_PAYR\n",
    "\t\t\t, LOWER(PROC_DESC)\n",
    "            , ed.\"Attend_Prov\"\n",
    "            , cds.dx\n",
    "            , cds.px\n",
    "            , cds.cpt\n",
    "            , cds.mod\n",
    "\t\t\t, RIGHT(REV_CODE, 3)\n",
    "\t\t\t, ubeditor.\"Recommendation\"\n",
    "\t\t\t, QUANTITY\n",
    "\t\t\t, AMOUNT\n",
    "            , LEFT(RIGHT(\"CPT_CODE\", 6), 5)\n",
    "            , CASE WHEN MODIFIER='' THEN NULL ELSE MODIFIER END\n",
    "\t) as t\n",
    "order by\n",
    "\thcpcs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(QUERY_2)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 ='''\n",
    "--Calulate average price for CPT/HCPCS Code\n",
    "SELECT\n",
    "\ta.service_date,\n",
    "    a.post_date,\n",
    "\ta.facility,\n",
    "\ta.account,\n",
    "\ta.patient_type,\n",
    "\ta.descrp,\n",
    "\tLEN(a.descrp) as descrp_length,\n",
    "    a.attending_provider,\n",
    "\ta.hcpcs,\n",
    "\tb.count_hcpcs,\n",
    "\ta.mod,\n",
    "\td.mod_count,\n",
    "\td.mod_count_quant,\n",
    "\td.mod_avg_quant,\n",
    "\td.mod_stdev_quant,\n",
    "\td.mod_max_quant,\n",
    "\td.mod_min_quant,\n",
    "\td.mod_var_quant,\n",
    "\td.mod_count_price,\n",
    "\td.mod_avg_price,\n",
    "\td.mod_stdev_price,\n",
    "\td.mod_max_price,\n",
    "\td.mod_min_price,\n",
    "\td.mod_var_price,\n",
    "\ta.source_ub04,\n",
    "\tc.count_source_ub04,\n",
    "\ta.quant,\n",
    "\tc.source_ub04_avg_quant,\n",
    "\tc.source_ub04_stdev_quant,\n",
    "\tc.source_ub04_max_quant,\n",
    "\tc.source_ub04_min_quant,\n",
    "\tb.hcpcs_count_quant,\n",
    "\tb.hcpcs_avg_quant,\n",
    "\tb.hcpcs_stdev_quant,\n",
    "\tb.hcpcs_max_quant,\n",
    "\tb.hcpcs_min_quant,\n",
    "\tb.hcpcs_var_quant,\n",
    "\ta.price,\n",
    "\tc.source_ub04_count_price,\n",
    "\tc.source_ub04_avg_price,\n",
    "\tc.source_ub04_stdev_price,\n",
    "\tc.source_ub04_max_price,\n",
    "\tc.source_ub04_min_price,\n",
    "\tc.source_ub04_var_price,\n",
    "\tb.hcpcs_count_price,\n",
    "\tb.hcpcs_avg_price,\n",
    "\tb.hcpcs_stdev_price,\n",
    "\tb.hcpcs_max_price,\n",
    "\tb.hcpcs_min_price,\n",
    "\tb.hcpcs_var_price,\n",
    "    a.target_ub04,\n",
    "    a.labels_dx,\n",
    "    a.labels_px,\n",
    "    a.labels_cpt,\n",
    "\ta.labels_mod\n",
    "from ##TEMP1 a\n",
    "left join\n",
    "\t(select\n",
    "\t\tHCPCS\n",
    "\t\t, count(hcpcs) as count_hcpcs\n",
    "\t\t, count(quant) as hcpcs_count_quant\n",
    "\t\t, avg(quant) as hcpcs_avg_quant\n",
    "\t\t, stdev(quant) as hcpcs_stdev_quant\n",
    "\t\t, max(quant) as hcpcs_max_quant\n",
    "\t\t, min(quant) as hcpcs_min_quant\n",
    "\t\t, var(quant) as hcpcs_var_quant\n",
    "\t\t, count(price) as hcpcs_count_price\n",
    "\t\t, avg(price) as hcpcs_avg_price\n",
    "\t\t, stdev(price) as hcpcs_stdev_price\n",
    "\t\t, max(price) as hcpcs_max_price\n",
    "\t\t, min(price) as hcpcs_min_price\n",
    "\t\t, var(price) as hcpcs_var_price\n",
    "\tfrom ##TEMP1\n",
    "\tGROUP BY\n",
    "\t\thcpcs\n",
    "\t) as b\n",
    "ON\n",
    "\ta.hcpcs = b.hcpcs\n",
    "left join\n",
    "\t(select\n",
    "\t\tsource_ub04\n",
    "\t\t, count(source_ub04) as count_source_ub04\n",
    "\t\t, count(quant) as source_ub04_count_quant\n",
    "\t\t, avg(quant) as source_ub04_avg_quant\n",
    "\t\t, stdev(quant) as source_ub04_stdev_quant\n",
    "\t\t, max(quant) as source_ub04_max_quant\n",
    "\t\t, min(quant) as source_ub04_min_quant\n",
    "\t\t, count(price) as source_ub04_count_price\n",
    "\t\t, avg(price) as source_ub04_avg_price\n",
    "\t\t, stdev(price) as source_ub04_stdev_price\n",
    "\t\t, max(price) as source_ub04_max_price\n",
    "\t\t, min(price) as source_ub04_min_price\n",
    "\t\t, var(price) as source_ub04_var_price\n",
    "\t from ##TEMP1\n",
    "\t group by\n",
    "\t\tsource_ub04\n",
    "\t) as c\n",
    "ON\n",
    "\ta.\"source_ub04\" = c.\"source_ub04\"\n",
    "\n",
    "left join\n",
    "\t(select\n",
    "\t\tmod\n",
    "\t\t, count(mod) as mod_count\n",
    "\t\t, count(quant) as mod_count_quant\n",
    "\t\t, avg(quant) as mod_avg_quant\n",
    "\t\t, stdev(quant) as mod_stdev_quant\n",
    "\t\t, max(quant) as mod_max_quant\n",
    "\t\t, min(quant) as mod_min_quant\n",
    "\t\t, var(quant) as mod_var_quant\n",
    "\t\t, count(price) as mod_count_price\n",
    "\t\t, avg(price) as mod_avg_price\n",
    "\t\t, stdev(price) as mod_stdev_price\n",
    "\t\t, max(price) as mod_max_price\n",
    "\t\t, min(price) as mod_min_price\n",
    "\t\t, var(price) as mod_var_price\n",
    "\tfrom ##TEMP1\n",
    "\tgroup by\n",
    "\t\tmod\n",
    "\t) as d\n",
    "ON\n",
    "\ta.\"mod\" = d.\"mod\"\n",
    "\t\t\n",
    "GROUP BY\n",
    "\ta.service_date,\n",
    "    a.post_date,\n",
    "\ta.facility,\n",
    "\ta.account,\n",
    "\ta.patient_type,\n",
    "\ta.descrp,\n",
    "    a.attending_provider,\n",
    "\ta.hcpcs,\n",
    "\tb.count_hcpcs,\n",
    "\ta.mod,\n",
    "\td.mod_count,\n",
    "\td.mod_count_quant,\n",
    "\td.mod_avg_quant,\n",
    "\td.mod_stdev_quant,\n",
    "\td.mod_max_quant,\n",
    "\td.mod_min_quant,\n",
    "\td.mod_var_quant,\n",
    "\td.mod_count_price,\n",
    "\td.mod_avg_price,\n",
    "\td.mod_stdev_price,\n",
    "\td.mod_max_price,\n",
    "\td.mod_min_price,\n",
    "\td.mod_var_price,\n",
    "\ta.source_ub04,\n",
    "\tc.count_source_ub04,\n",
    "\ta.quant,\n",
    "\tc.source_ub04_avg_quant,\n",
    "\tc.source_ub04_stdev_quant,\n",
    "\tc.source_ub04_max_quant,\n",
    "\tc.source_ub04_min_quant,\n",
    "\tb.hcpcs_count_quant,\n",
    "\tb.hcpcs_avg_quant,\n",
    "\tb.hcpcs_stdev_quant,\n",
    "\tb.hcpcs_max_quant,\n",
    "\tb.hcpcs_min_quant,\n",
    "\tb.hcpcs_var_quant,\n",
    "\ta.price,\n",
    "\tc.source_ub04_count_price,\n",
    "\tc.source_ub04_avg_price,\n",
    "\tc.source_ub04_stdev_price,\n",
    "\tc.source_ub04_max_price,\n",
    "\tc.source_ub04_min_price,\n",
    "\tc.source_ub04_var_price,\n",
    "\tb.hcpcs_count_price,\n",
    "\tb.hcpcs_avg_price,\n",
    "\tb.hcpcs_stdev_price,\n",
    "\tb.hcpcs_max_price,\n",
    "\tb.hcpcs_min_price,\n",
    "\tb.hcpcs_var_price,\n",
    "    a.target_ub04,\n",
    "    a.labels_dx,\n",
    "    a.labels_px,\n",
    "    a.labels_cpt,\n",
    "\ta.labels_mod\n",
    "order by\n",
    "\ta.service_date asc\n",
    "\t, account asc\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(QUERY_3, connection)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up labels columns\n",
    "df1['labels_dx'] = df1['labels_dx'].str.split()\n",
    "df1['labels_px'] = df1['labels_px'].str.split()\n",
    "df1['labels_cpt'] = df1['labels_cpt'].str.split()\n",
    "df1['labels_mod'] = df1['labels_mod'].str.split()\n",
    "df1['source_ub04'] = df1['source_ub04'].str.split()\n",
    "p_cols = ['source_ub04','labels_dx','labels_px','labels_cpt','labels_mod']\n",
    "df1[p_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean Up Target UB04 columns\n",
    "df1['target_ub04'] = df1['target_ub04'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "numbers_list = []\n",
    "for string in df1['target_ub04'].astype(str):\n",
    "    numbers = [int(s) for s in string.split() if s.isdigit()]\n",
    "    numbers_list.append(numbers)\n",
    "    \n",
    "df1['target_ub04'] = numbers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['labels_dx'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting the labels into a matrix, the size of the matrix can overload the 16 GB ram computer.\n",
    "To keep the memory size down, will exlude px codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['labels_combined'] = df1['source_ub04'] +df1['labels_dx']+df1['labels_cpt']+df1['labels_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['labels_combined'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medicare MUE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://data.medicaid.gov/api/views/8pny-kgh5/rows.csv?accessType=DOWNLOAD'\n",
    "mue=pd.read_csv(url)\n",
    "mue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data\n",
    "mue_grouped = mue.groupby('HCPCS/CPT Code').agg(['count', 'sum','mean','median','std','min', 'max', 'var'])\n",
    "mue_grouped.columns = [' '.join(col).strip().lower() for col in mue_grouped.columns.values]\n",
    "mue_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "df2 = df1.set_index('hcpcs')\n",
    "print(df2.shape[0])\n",
    "df2 = df2.merge(mue_grouped, how='left', left_index=True, right_index=True)\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medicare Alpha-Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# taken out for now, doesn't help much\n",
    "\n",
    "# download, unzip and load files\n",
    "import requests, zipfile, io\n",
    "\n",
    "zip_file_url = 'https://www.cms.gov/Medicare/Coding/HCPCSReleaseCodeSets/Downloads/2018-Alpha-Numeric-HCPCS-File.zip'\n",
    "\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "alpa_numeric_df = pd.read_excel('HCPC2018_CONTR_ANWEB_disc.xlsx', skiprows=10)\n",
    "\n",
    "\n",
    "# clean up alpa_numeric\n",
    "drop_cols = ['SEQNUM',\n",
    "    'RECID',\n",
    "    'PRICE2',\n",
    "    'PRICE3',\n",
    "    'PRICE4',\n",
    "    'CIM2',\n",
    "    'CIM3',\n",
    "    'MCM2',\n",
    "    'MCM3',\n",
    "    'TERM DT',\n",
    "    'LABCERT2',\n",
    "    'LABCERT3',\n",
    "    'LABCERT4',\n",
    "    'LABCERT5',\n",
    "    'LABCERT6',\n",
    "    'LABCERT7',\n",
    "    'LABCERT8',\n",
    "    'XREF2',\n",
    "    'XREF3',\n",
    "    'XREF4',\n",
    "    'XREF5',\n",
    "    'OPPS',\n",
    "    'OPPS_PI',\n",
    "    'OPPS_DT',\n",
    "    'TOS2',\n",
    "    'TOS3',\n",
    "    'TOS4',\n",
    "    'TOS5' \n",
    "         ]\n",
    "\n",
    "alpa_numeric_df = pd.read_excel('HCPC2018_CONTR_ANWEB_disc.xlsx', skiprows=10)\n",
    "alpa_numeric_df = alpa_numeric_df.drop(columns=drop_cols)\n",
    "alpa_numeric_df['LONG DESCRIPTION'] = alpa_numeric_df['LONG DESCRIPTION'].str.len()\n",
    "alpa_numeric_df['SHORT DESCRIPTION'] = alpa_numeric_df['SHORT DESCRIPTION'].str.len()\n",
    "alpa_numeric_df['ADD DT'] = pd.to_datetime(alpa_numeric_df['ADD DT'],format='%Y%m%d')\n",
    "alpa_numeric_df['ACT EFF DT'] = pd.to_datetime(alpa_numeric_df['ACT EFF DT'],format='%Y%m%d')\n",
    "alpa_numeric_df = alpa_numeric_df.set_index('HCPC')\n",
    "alpa_numeric_df.head(3)\n",
    "\n",
    "# merge data\n",
    "print(df2.shape[0])\n",
    "df2 = df2.merge(alpa_numeric_df, how='left', left_index=True, right_index=True)\n",
    "print(df2.shape[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medicare RVU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ State = Massachusetts\n",
    "+ Fee Schedule Areas = METROPOLITAN BOSTON\n",
    "+ Carrier # = 14212\n",
    "+ Locality Number = 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download, unzip and load files\n",
    "import requests, zipfile, io\n",
    "\n",
    "zip_file_url = 'https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/PhysicianFeeSched/Downloads/RVU18C1.zip'\n",
    "\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medrvu = pd.read_excel('PPRRVU18_JUL.xlsx', skiprows=9)\n",
    "medrvu = medrvu.set_index('HCPCS')\n",
    "medrvu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "print(df2.shape[0])\n",
    "df2 = df2.merge(medrvu, how='left', left_index=True, right_index=True)\n",
    "print(df2.shape[0])\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oppscap_df = pd.read_excel('OPPSCAP_JUL.xlsx')\n",
    "oppscap_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oppscap_df = oppscap_df[(oppscap_df['LOCALITY'] == 1.0) & (oppscap_df['CARIER'] == 1212.0)]\n",
    "oppscap_no_mod_df = oppscap_df[oppscap_df['MOD'].isnull()]\n",
    "oppscap_yes_mod_df = oppscap_df[oppscap_df['MOD'].notnull()]\n",
    "\n",
    "oppscap_no_mod_df = oppscap_no_mod_df.set_index('HCPCS')\n",
    "oppscap_yes_mod_df = oppscap_yes_mod_df.set_index('HCPCS')\n",
    "\n",
    "\n",
    "df2_no_mod = df2[df2['mod'].isnull()]\n",
    "df2_yes_mod = df2[df2['mod'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2_no_mod.shape[0])\n",
    "df2_no_mod = df2_no_mod.merge(oppscap_no_mod_df, how='left', left_index=True, right_index=True).reset_index()\n",
    "print(df2_no_mod.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2_yes_mod.shape[0])\n",
    "df2_yes_mod = df2_yes_mod.merge(oppscap_yes_mod_df, how='left', left_index=True, right_index=True).reset_index()\n",
    "print(df2_yes_mod.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df2_no_mod, df2_yes_mod])\n",
    "df3.rename(columns={'index':'hcpcs'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sort_values(by=['service_date','account'], inplace=True)\n",
    "#df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.groupby('BASE').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "'service_date',\n",
    "'post_date',\n",
    "'account',\n",
    "'patient_type',\n",
    "'attending_provider',\n",
    "'descrp',\n",
    "'target_ub04',\n",
    "'DESCRIPTION',\n",
    "'CODE',\n",
    "'PAYMENT',\n",
    "'IND',\n",
    "'DAYS',\n",
    "'MOD_x',\n",
    "'MOD_y',\n",
    "'PROCSTAT',\n",
    "'CARIER',\n",
    "'LOCALITY',\n",
    "'OP',\n",
    "'OP.1',\n",
    "'OP.2',\n",
    "'PROC',\n",
    "'SURG',\n",
    "'SURG.1',\n",
    "'SURG.2',\n",
    "'SURG.3',\n",
    "'BASE',\n",
    "'FACTOR',\n",
    "'PROCEDURES',\n",
    "'FLAG',\n",
    "'INDICATOR.2',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['descrp_length',\n",
    "'count_hcpcs',\n",
    "'mod_count',\n",
    "'mod_count_quant',\n",
    "'mod_avg_quant',\n",
    "'mod_stdev_quant',\n",
    "'mod_max_quant',\n",
    "'mod_min_quant',\n",
    "'mod_var_quant',\n",
    "'mod_count_price',\n",
    "'mod_avg_price',\n",
    "'mod_stdev_price',\n",
    "'mod_max_price',\n",
    "'mod_min_price',\n",
    "'mod_var_price',\n",
    "'count_source_ub04',\n",
    "'quant',\n",
    "'source_ub04_avg_quant',\n",
    "'source_ub04_stdev_quant',\n",
    "'source_ub04_max_quant',\n",
    "'source_ub04_min_quant',\n",
    "'hcpcs_count_quant',\n",
    "'hcpcs_avg_quant',\n",
    "'hcpcs_stdev_quant',\n",
    "'hcpcs_max_quant',\n",
    "'hcpcs_min_quant',\n",
    "'hcpcs_var_quant',\n",
    "'price',\n",
    "'source_ub04_count_price',\n",
    "'source_ub04_avg_price',\n",
    "'source_ub04_stdev_price',\n",
    "'source_ub04_max_price',\n",
    "'source_ub04_min_price',\n",
    "'source_ub04_var_price',\n",
    "'hcpcs_count_price',\n",
    "'hcpcs_avg_price',\n",
    "'hcpcs_stdev_price',\n",
    "'hcpcs_max_price',\n",
    "'hcpcs_min_price',\n",
    "'hcpcs_var_price',\n",
    "'mue value count',\n",
    "'mue value sum',\n",
    "'mue value mean',\n",
    "'mue value median',\n",
    "'mue value std',\n",
    "'mue value min',\n",
    "'mue value max',\n",
    "'mue value var',\n",
    "'RVU',\n",
    "'PE RVU',\n",
    "'PE RVU.1',\n",
    "'TOTAL',\n",
    "'TOTAL.1',\n",
    "'AMOUNT',\n",
    "'AMOUNT.1',\n",
    "'AMOUNT.2',\n",
    "'FACILITY PRICE',\n",
    "'NON-FACILTY PRICE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cols = ['labels_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['facility','INDICATOR', 'INDICATOR.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.sample(100000, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = df4[cat_cols].reset_index(drop=True)\n",
    "num_df = df4[numerical_cols].reset_index(drop=True)\n",
    "labels_df = df4[labels_cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['labels_combined'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = cat_df.columns.tolist()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    cat_df[col] = labelencoder_X_1.fit_transform(cat_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = cat_df.index.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(cat_df).toarray()\n",
    "#index = [str(i) for i in range(1, len(X)+1)]\n",
    "cat_hot = pd.DataFrame(X, index=index)\n",
    "cat_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_hot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = num_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_hot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cat_df\n",
    "del df4\n",
    "del df3\n",
    "del df2\n",
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_hot.to_csv(\"cat_hot.zip\", index_label=False, chunksize=10000, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cat_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.to_csv(\"num_df.zip\", index_label=False, chunksize=10000, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.to_csv(\"labels_df.zip\", index_label=False, chunksize=10000, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Restart the Kernel for memory reasons and begin running from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"cat_hot.zip\", nrows=3, compression='zip')\n",
    "df2 = pd.read_csv(\"num_df.zip\", nrows=3, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a empty bucket to save result\n",
    "df_result = pd.DataFrame(columns=(df1.columns.append(df2.columns)).unique())\n",
    "df_result.to_csv('features.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting df2 to save memory\n",
    "del(df2)\n",
    "del(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    df2=pd.merge(df1,x,how='inner', left_index=True, right_index=True)\n",
    "    df2.to_csv(\"features.csv\",mode=\"a\",header=False,index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(\"cat_hot.zip\", chunksize=1000, compression='zip') # chunksize depends with you colsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('num_df.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[preprocess(r) for r in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = cat_hot.merge(num_df, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Labels (Restart Kernel Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"labels_df.zip\", compression='zip')\n",
    "\n",
    "from ast import literal_eval\n",
    "new_list = []\n",
    "s = labels_df['labels_combined']\n",
    "for i in s:\n",
    "    new_string = i\n",
    "    #print(literal_eval(new_string))\n",
    "    new_list.append(literal_eval(new_string))\n",
    "\n",
    "labels_df['labels_combined'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_list\n",
    "del s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['labels_combined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = labels_df['labels_combined'].tolist()\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rows \",y.shape[0])\n",
    "print(\"columns \",y.shape[1])\n",
    "print(\"cells \", y.shape[0]*y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = list(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "hkl.dump(X_train, 'X_train.hkl' )\n",
    "hkl.dump(X_test, 'X_test.hkl' )\n",
    "hkl.dump(y_test, 'y_test.hkl' )\n",
    "hkl.dump(y_train, 'y_train.hkl' )\n",
    "hkl.dump(labels_list, 'labels_list.hkl' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Testing Machine Learning Models\n",
    "You can start from here if you don't need to make any adjustments to the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Machine Gaussian Naive Bayes Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "X_train = hkl.load( 'X_train.hkl' )\n",
    "X_test = hkl.load( 'X_test.hkl' )\n",
    "y_test = hkl.load( 'y_test.hkl' )\n",
    "y_train = hkl.load( 'y_train.hkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classif_list = []\n",
    "train_score_list, test_score_list = [], []\n",
    "\n",
    "# Training\n",
    "print(\"Training\")\n",
    "for i in range(len(labels_list)):\n",
    "    y_train_for_this_ub04 = y_train[:,i]\n",
    "    new_classifier = GaussianNB()\n",
    "    new_classifier.fit(X_train, y_train_for_this_ub04)\n",
    "    classif_list.append(new_classifier)\n",
    "\n",
    "# Test & display results\n",
    "print(\"Test and display results\")\n",
    "for i in range(len(labels_list)):\n",
    "    classif = classif_list[i]\n",
    "    train_score = float('{0:.3f}'.format(classif.score(X_train, y_train[:,i])))\n",
    "    test_score = float('{0:.3f}'.format(classif.score(X_test, y_test[:,i])))\n",
    "    train_score_list.append(train_score)\n",
    "    test_score_list.append(test_score)\n",
    "    print('Detecting {} with {}% accuracy (training {}%)'.format(labels_list[i], 100*test_score, 100*train_score))\n",
    "\n",
    "predict_train = np.zeros_like(y_train)\n",
    "predict_test = np.zeros_like(y_test)\n",
    "for i in range(len(labels_list)):\n",
    "    classif = classif_list[i]\n",
    "    predict_train[:,i] = classif.predict(X_train)\n",
    "    predict_test[:,i] = classif.predict(X_test)\n",
    "acc_train = 1 - np.sum(np.abs(predict_train - y_train))/(y_train.shape[0]*y_train.shape[1])\n",
    "acc_test = 1 - np.sum(np.abs(predict_test - y_test))/(y_test.shape[0]*y_test.shape[1])\n",
    "print('###')\n",
    "print('Global accuracy: testing {}, training {}'.format(acc_test, acc_train))\n",
    "\n",
    "well_labeled = 0\n",
    "for i in range(len(y_train)):\n",
    "    if np.sum(np.abs(y_train[i,:] - predict_train[i,:])) == 0:\n",
    "        well_labeled +=1\n",
    "print('Overall {} out of the {} training samples were well labeled'.format(well_labeled,len(y_train)))\n",
    "\n",
    "well_labeled = 0\n",
    "for i in range(len(y_test)):\n",
    "    if np.sum(np.abs(y_test[i,:] - predict_test[i,:])) == 0:\n",
    "        well_labeled +=1\n",
    "print('Overall {} out of the {} testing samples were well labeled'.format(well_labeled,len(y_test)))\n",
    "\n",
    "# Evidemment, en plus d'être mathématiquement désapprouvée, cette méthode est encombrante:\n",
    "# imaginez avoir à construire un million de classificateurs ! Ce n'est pas du tout extensible à de la classification extrême"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Y Matrix\n",
    "Follwing XML method in the paper [Deep Extreme Multi-label Learning by Zhang et al](https://arxiv.org/pdf/1704.03718.pdf) and [eXtreme Multilabel Classification Notebook](https://github.com/therhappy/xml-tuto/blob/master/eXtreme%20Multilabel%20Classification%20Notebook%20-%20EN.ipynb). \n",
    "\n",
    "The goal in this section is build an embedded vector of the UB-04, ICD-10, CPT and Modifier labels. This embedded vector will be used to predict the correct label for EHR procedures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "y_train = hkl.load( 'y_train.hkl' )\n",
    "labels_list = hkl.load('labels_list.hkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10022', '10060', '10120', '10160', '10180']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lables names:  8630\n"
     ]
    }
   ],
   "source": [
    "print(\"lables names: \", len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67000\n",
      "8630\n"
     ]
    }
   ],
   "source": [
    "#y_train = y_train[0:1000]\n",
    "print(y_train.shape[0])\n",
    "print(y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67000\n",
      "8630\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "matrix = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "print(matrix.shape[0])\n",
    "print(matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for row in y_train:\n",
    "    act = list(np.where(row == 1))\n",
    "    act = [list(i) for i in act ][0]\n",
    "    for i in range(len(act)):\n",
    "        for j in range(len(act)):\n",
    "            matrix[i,act[j]] +=1\n",
    "            \n",
    "            #if ([act[i],act[j]] not in edges) and ([act[j],act[i]] not in edges): #and not i==j:\n",
    "            #if [act[j],act[i]] not in edges and not i==j:\n",
    "            #if not i==j and i <:\n",
    "            if i < j:\n",
    "                edges.append([act[i],act[j]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0][1914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,  10.,   1., ..., 267., 613.,  47.],\n",
       "       [  1.,  10.,   1., ..., 267., 613.,  47.],\n",
       "       [  1.,  10.,   1., ..., 267., 613.,  47.],\n",
       "       ...,\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3612991"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2 = [[x,y] for x,y in (set(tuple(x) for x in edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[292, 8167], [2538, 2974], [959, 3912], [1559, 1957], [3897, 4036]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for i in range(len(labels_list)):\n",
    "    label_dict[i] = labels_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(edges2)):\\n    val1 = edges2[i][0]\\n    val2 = edges2[i][1]\\n    edges2[i][0] = label_dict[val1]\\n    edges2[i][1] = label_dict[val2]\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(len(edges2)):\n",
    "    val1 = edges2[i][0]\n",
    "    val2 = edges2[i][1]\n",
    "    edges2[i][0] = label_dict[val1]\n",
    "    edges2[i][1] = label_dict[val2]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[292, 8167],\n",
       " [2538, 2974],\n",
       " [959, 3912],\n",
       " [1559, 1957],\n",
       " [3897, 4036],\n",
       " [328, 8279],\n",
       " [6208, 6330],\n",
       " [1978, 7968],\n",
       " [3462, 6113],\n",
       " [1568, 3416]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "norm_matrix = normalize(matrix, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "for edge in edges2:\n",
    "        G.add_edge(edge[0], edge[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.93574488e-06, 1.93574488e-05, 1.93574488e-06, ...,\n",
       "        5.16843884e-04, 1.18661161e-03, 9.09800096e-05],\n",
       "       [1.93913445e-06, 1.93913445e-05, 1.93913445e-06, ...,\n",
       "        5.17748898e-04, 1.18868942e-03, 9.11393191e-05],\n",
       "       [2.07942575e-06, 2.07942575e-05, 2.07942575e-06, ...,\n",
       "        5.55206674e-04, 1.27468798e-03, 9.77330101e-05],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx #For those among you that use Anaconda dist, you already got this\n",
    "\n",
    "def draw_graph(edges, weights_matrix=None, threshold=0, figsize=(20,20)):\n",
    "    \n",
    "    '''\n",
    "    edges : liste des connected labels\n",
    "    weights_matrix : labels proximity matrix\n",
    "    threshold : number of occurence required to draw a connection\n",
    "    figsize : size of the graph to display\n",
    "    '''\n",
    "    \n",
    "    edges = [edge for edge in edges if weights_matrix[edge[0],edge[1]] > threshold]\n",
    "    \n",
    "    # additional settings (you can mess around here)\n",
    "    node_size = 1600\n",
    "    node_color = 'blue'\n",
    "    node_alpha = 0.2\n",
    "    node_text_size = 12\n",
    "    edge_color = 'blue'\n",
    "    edge_alpha= 0.3\n",
    "    edge_tickness = 1\n",
    "    edge_text_pos = 0.3\n",
    "    text_font = 'sans-serif'\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    # create networkx graph\n",
    "    G=nx.Graph()\n",
    "\n",
    "    # add edges\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # select shell autolocation\n",
    "    graph_pos=nx.shell_layout(G)\n",
    "\n",
    "    # draw graph\n",
    "    nx.draw_networkx_nodes(G,\n",
    "                           graph_pos,\n",
    "                           node_size=node_size, \n",
    "                           alpha=node_alpha,\n",
    "                           node_color=node_color)\n",
    "    nx.draw_networkx_edges(G, graph_pos,width=edge_tickness, alpha=edge_alpha,edge_color=edge_color)\n",
    "    nx.draw_networkx_labels(G, graph_pos,font_size=node_text_size, font_family=text_font)\n",
    "    \n",
    "    # construct weights dict\n",
    "    weights={}\n",
    "    for i in range(len(edges)):\n",
    "        weights[tuple(edges[i])] = weights_matrix.astype(int)[edges[i][0],edges[i][1]]\n",
    "    \n",
    "    # draw weights\n",
    "    edge_labels = weights\n",
    "    nx.draw_networkx_edge_labels(G, graph_pos, edge_labels=edge_labels, \n",
    "                                 label_pos=edge_text_pos)\n",
    "\n",
    "    # show graph\n",
    "    plt.title('Proximity graph between labels with a proximity threshold at {} occurences'.format(threshold))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "num_nodes = norm_matrix.shape[0] + norm_matrix.shape[1]\n",
    "rows, cols = np.where(norm_matrix == 1)\n",
    "edges = list(zip(rows.tolist(), (cols + norm_matrix.shape[0]).tolist()))\n",
    "print(\"X:\", norm_matrix)\n",
    "print(\"U nodes:\", np.arange(norm_matrix.shape[0]))\n",
    "print(\"V nodes:\", np.arange(norm_matrix.shape[1]) + norm_matrix.shape[0])\n",
    "print(\"edges\")\n",
    "print(edges[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.draw_networkx(b, pos=pos, node_color=(['c'] * norm_matrix.shape[0]) + (['y'] * norm_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjacency list\n",
    "#nx.write_edgelist(b, \"test.edgelist.txt\", delimiter='\\t', data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as edge list file\n",
    "with open('./test.edgelist', 'w') as f:\n",
    "    for edge in G.edges():\n",
    "        f.write(\"{} {}\\n\".format(edge[0] ,edge[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(b, \"test.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the graph in GraphCrunch 2\n",
    "+ [GraphCrunch 2](http://www0.cs.ucl.ac.uk/staff/natasa/graphcrunch2/index.html)\n",
    "+ [Paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose How to Convert to Vec\n",
    "There are four methods to convert a network graph each node into a network as a low-dimensional feature vector.\n",
    "\n",
    "1. [Deepwalk](https://github.com/phanein/deepwalk)\n",
    "2. [Node2Vec](https://snap.stanford.edu/node2vec/)\n",
    "3. [Struct2Vec](https://github.com/leoribeiro/struc2vec)\n",
    "4. Graphlets\n",
    "\n",
    "Due a paper from Shawn Gu and Tijana Milenković called [*Graphlets versus node2vec and struc2vec in the task of network alignment*](https://www.groundai.com/project/graphlets-versus-node2vec-and-struc2vec-in-the-task-of-network-alignment/), in certain situations a graphlet could outperform Node2Vec or Struct2Vec to quantify node similarities.\n",
    "\n",
    "In this section two embedded graphs will be made. One, using Node2Vec and the second using Graphlets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* :To use Node2Vec, you must use a Python 2.7 environment at the time of writing this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2Vec\n",
    "To run NodeVec, you first must download the repository and run the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python src/main.py --input graph/test.edgelist --output emb/test.emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
